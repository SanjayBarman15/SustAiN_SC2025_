{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/synthetic_flood_data.csv\")\n",
    "print(\"âœ… Dataset Loaded Successfully!\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert binary categorical columns (\"Yes\"/\"No\") to numerical (0/1)\n",
    "binary_cols = [\"Road Flooding Status\"]\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map({\"No\": 0, \"Yes\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns (handling missing columns dynamically)\n",
    "categorical_cols = [\"Flood Severity\", \"Land Use Type\"]\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)  # Avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Flood Occurrence (1/0)\"])\n",
    "y = df[\"Flood Occurrence (1/0)\"]\n",
    "\n",
    "# Normalize data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "print(\"âœ… Data Resampled using SMOTE!\")\n",
    "\n",
    "# Reshape input for LSTM (samples, timesteps, features)\n",
    "X_resampled = X_resampled.reshape(X_resampled.shape[0], 1, X_resampled.shape[1])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "print(f\"âœ… Training Size: {X_train.shape}, Test Size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build LSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[2]))),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32, return_sequences=False)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nâœ… Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Save Model\n",
    "model.save(\"/content/lstm_flood_model.h5\")\n",
    "print(\"\\nâœ… Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Flood\", \"Flood\"], yticklabels=[\"No Flood\", \"Flood\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nðŸ”¹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Accuracy & Loss Over Epochs\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy Bar Graph\n",
    "ax[0].bar(range(len(history.history[\"accuracy\"])), history.history[\"accuracy\"], color=\"blue\", label=\"Training Accuracy\", alpha=0.7)\n",
    "ax[0].bar(range(len(history.history[\"val_accuracy\"])), history.history[\"val_accuracy\"], color=\"orange\", label=\"Validation Accuracy\", alpha=0.7)\n",
    "ax[0].set_title(\"Training vs Validation Accuracy\")\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Loss Bar Graph\n",
    "ax[1].bar(range(len(history.history[\"loss\"])), history.history[\"loss\"], color=\"red\", label=\"Training Loss\", alpha=0.7)\n",
    "ax[1].bar(range(len(history.history[\"val_loss\"])), history.history[\"val_loss\"], color=\"green\", label=\"Validation Loss\", alpha=0.7)\n",
    "ax[1].set_title(\"Training vs Validation Loss\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
